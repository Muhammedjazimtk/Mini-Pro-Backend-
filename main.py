# -*- coding: utf-8 -*-
"""BOTZ(Tensorflow Model).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgXFp2iWAcuW7wzcvvrAeMpfMRro4qHv
"""
from sklearn.preprocessing import LabelEncoder
import json
from fastapi import FastAPI,Body,Response,status
import string
import tensorflow as tf
import numpy as np
import pandas as pd
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input,Embedding,LSTM,Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
import random
from pydantic import BaseModel, Json
app=FastAPI()


class Predict(BaseModel):
  train_data : dict
  query : str


@app.get("/")
def root():
  return {"success" : "True"}


# with open("intents.json") as content:
#         data1=json.load(content)
# print(data1)

#importing the libraries

@app.post("/predict")
def train(train_data: Predict):
  tags=[]
  inputs=[]
  responses={}
  model=[]
  train_data=train_data.dict()
  
  print(train_data)
  data1=train_data['train_data']
  

  
  for intents in data1['intents']:
    responses[intents['tag']]=intents['responses']
    for lines in intents['patterns']:
        inputs.append(lines)
        tags.append(intents['tag'])

  print(tags , inputs )

    #converting the dataframe
  data=pd.DataFrame({"inputs":inputs,"tags":tags})
  print(data)

  # #preprocessing the data

  # #removing punctuations
  data['inputs']=data['inputs'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
  data['inputs']=data['inputs'].apply(lambda wrd:''.join(wrd))




  tokenizer=Tokenizer(num_words=2000)
  tokenizer.fit_on_texts(data['inputs'])
  train=tokenizer.texts_to_sequences(data['inputs'])


  x_train=pad_sequences(train)

  # #encoding the outputs

  le=LabelEncoder()

  y_train=le.fit_transform(data['tags'])

  print(y_train)

  # #input length
  input_shape=x_train.shape[1]

  # #define vocabulary

  vocabulary=len(tokenizer.word_index)
  print("Number of Unique Words:",vocabulary)

  # #output length

  output_length=le.classes_.shape[0]
  print(output_length)

  #Neural Networks


  print(data)
  i=Input(shape=input_shape)
  x=Embedding(vocabulary+1,10)(i)
  x=LSTM(10,return_sequences=True)(x)
  x=Flatten()(x)
  x=Dense(output_length,activation="softmax")(x)
  model=Model(i,x)

  # #compiling the model
  model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])


  # #training the model
  train=model.fit(x_train,y_train,epochs=200)
  texts_p=[]


  prediction_input=train_data['query']
  

  # removing punctuation and converting to lowercase
  print(prediction_input)
  prediction_input=[letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input=('').join(prediction_input)


  texts_p.append(prediction_input)

  #   #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input=np.array(prediction_input).reshape(-1)
  prediction_input=pad_sequences([prediction_input],input_shape)

  #   #getting putput
    
  output=model.predict(prediction_input)
  output=output.argmax()
  response_tag=le.inverse_transform([output])[0]

    
    
  return {'data':random.choice(responses[response_tag])}



# @app.post('/train')
# def train():
#   pass  

